# -*- coding: utf-8 -*-
"""Model Menuju Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13kXz0Rama_evSHV7ylZvru7pmp1eLRCg

# **Model 1: Skoring Kecocokan CV-JD (TensorFlow)**

## **Import Library**
"""

# Import Library & Instalasi
!pip install pandas torch sentence-transformers scikit-learn tensorflow "imblearn" -q

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sentence_transformers import SentenceTransformer, util
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout
from imblearn.over_sampling import SMOTE
import warnings

# Mengunduh resource NLTK yang diperlukan
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('corpora/wordnet.zip')
except LookupError:
    nltk.download('wordnet')
    nltk.download('omw-1.4')

print("Library dan NLTK resources siap.")

"""# **Load Dataset**"""

# Dataset CV
file_id1 = '1rwbz0ztKjfASS0--uO2SBsq0bP4rIYj9'
url1 = f'https://drive.google.com/uc?id={file_id1}'
df_resume = pd.read_csv(url1, sep=',')

print("Dataset berhasil dimuat.")
print(f"Jumlah baris awal: {len(df_resume)}")
df_resume.head()

# Dataset Pekerjaan
file_id2 = '1i4-MAdJDgl4pxSRUr3SwEvs7MrGxM_7o'
url2 = f'https://drive.google.com/uc?id={file_id2}'
df_jd = pd.read_csv(url2, sep=',')

print("Dataset berhasil dimuat.")
print(f"Jumlah baris awal: {len(df_jd)}")
df_jd.head()

# Mengganti nama kolom agar konsisten
df_jd.rename(columns={'Job Id': 'Job_ID', 'skills': 'Skills'}, inplace=True)

print("Dataset CV dan Lowongan Pekerjaan berhasil dimuat.")
print(f"Jumlah data CV: {len(df_resume)}")
print(f"Jumlah data Lowongan: {len(df_jd)}")

# Inspeksi Data (Untuk mengetahui nama kolom)
print("Info Job Description (JD):")
df_jd.info()
print("\nInfo Resume:")
df_resume.info()

"""# **Data Preparation**"""

# Menggabungkan Teks untuk Dataset CV

# Mengisi nilai NaN (kosong) dengan string kosong '' agar tidak error saat digabung.
# Kolom yang relevan untuk CV
cv_text_cols = ['Category', 'Summary', 'Certifications', 'Skills', 'Experience', 'Education', 'Projects']
for col in cv_text_cols:
    df_resume[col] = df_resume[col].fillna('')

# Menggabungkan kolom-kolom tersebut menjadi satu kolom baru 'CV_text'
df_resume['CV_text'] = df_resume[cv_text_cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)


# Menggabungkan Teks untuk Dataset Lowongan Pekerjaan

# Mengisi nilai NaN (kosong) dengan string kosong ''
# Kolom yang relevan untuk Lowongan
jd_text_cols = ['Role', 'Job Description', 'Skills', 'Responsibilities']
for col in jd_text_cols:
    df_jd[col] = df_jd[col].fillna('')

# Menggabungkan kolom-kolom tersebut menjadi satu kolom baru 'JD_text'
df_jd['JD_text'] = df_jd[jd_text_cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)


# Melihat hasil
print("--- Contoh Teks CV Gabungan ---")
print(df_resume[['CV_text']].head())
print("\n" + "="*50 + "\n")
print("--- Contoh Teks Lowongan Pekerjaan Gabungan ---")
print(df_jd[['JD_text']].head())

# Pra-pemrosesan Teks

# Menggunakan stopwords untuk Bahasa Inggris
stop_words_english = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text_improved(text):
    # Pastikan input adalah string
    if not isinstance(text, str):
        return ""

    # Mengubah ke huruf kecil
    text = text.lower()

    # Menghapus URL
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Menghapus karakter non-alfabet (mempertahankan huruf saja)
    text = re.sub(r'[^a-z\s]', '', text)

    # Tokenisasi (memecah teks menjadi kata)
    words = text.split()

    # Menghapus stopwords Bahasa Inggris dan lakukan Lemmatization
    filtered_lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words_english]

    # Menggabungkan kembali menjadi teks
    return " ".join(filtered_lemmatized_words)

# Menerapkan fungsi pra-pemrosesan ke kolom yang benar
print("Menerapkan fungsi pra-pemrosesan ke dataset...")
df_resume['cleaned_text'] = df_resume['CV_text'].apply(preprocess_text_improved)
df_jd['cleaned_text'] = df_jd['JD_text'].apply(preprocess_text_improved)
print("Pra-pemrosesan selesai.")

# Melihat hasil
print("\n" + "="*50 + "\n")
print("--- Contoh Hasil Pembersihan Teks pada CV ---")
# Menampilkan teks sebelum dan sesudah dibersihkan untuk satu contoh
print("SEBELUM:", df_resume['CV_text'][0])
print("\nSESUDAH:", df_resume['cleaned_text'][0])

!pip install pandas torch sentence-transformers scikit-learn numpy -q

# Feature Engineering dengan SentenceTransformer
print("Memuat model SentenceTransformer untuk membuat embeddings...")
# Model ini digunakan sebagai feature extractor (transfer learning), bukan model klasifikasi jadi
model_sbert = SentenceTransformer('all-MiniLM-L6-v2')

# Membuat embeddings untuk semua data CV dan JD yang sudah bersih
df_resume_sample = df_resume.sample(n=500, random_state=42)
df_jd_sample = df_jd.sample(n=150, random_state=42)

print(f"Membuat embeddings untuk {len(df_resume_sample)} CV dan {len(df_jd_sample)} JD...")
resume_embeddings = model_sbert.encode(df_resume_sample['cleaned_text'].tolist(), show_progress_bar=True)
jd_embeddings = model_sbert.encode(df_jd_sample['cleaned_text'].tolist(), show_progress_bar=True)
print("Embeddings berhasil dibuat.")

# Membuat Dataset Pelatihan untuk Model Regresi (Skoring)

print("Membuat dataset pasangan CV-JD untuk pelatihan model skoring...")
pairs_for_scoring = []
cosine_scores = util.cos_sim(resume_embeddings, jd_embeddings)

for i in range(len(resume_embeddings)):
    for j in range(len(jd_embeddings)):
        # Selisih embeddings)
        feature_vector = resume_embeddings[i] - jd_embeddings[j]

        # SKOR KEMIRIPAN ASLI
        target_score = cosine_scores[i][j].item()

        # Pastikan skor berada dalam rentang [0, 1] jika ada nilai negatif
        target_score = max(0, target_score)

        pairs_for_scoring.append({'features': feature_vector, 'target_score': target_score})

df_training_scoring = pd.DataFrame(pairs_for_scoring)

print(f"Dataset pelatihan untuk skoring berhasil dibuat dengan {len(df_training_scoring)} pasangan.")
print("Contoh data:")
print(df_training_scoring.head())

# Pisahkan fitur (X) dan target skor (y)
X_scoring = np.array(df_training_scoring['features'].tolist())
y_scoring = np.array(df_training_scoring['target_score'].tolist())

# Memisahkan data menjadi data latih dan data uji
X_train_score, X_test_score, y_train_score, y_test_score = train_test_split(
    X_scoring, y_scoring, test_size=0.2, random_state=42
)

print(f"\nData siap untuk dilatih. Ukuran data latih: {X_train_score.shape[0]}")

# Membangun, Melatih, dan Mengevaluasi Model Skoring TensorFlow

print("\nMembangun model skoring (regresi) dengan TensorFlow...")

# Mendapatkan dimensi input dari bentuk fitur
input_dim = X_train_score.shape[1]

# Mendefinisikan arsitektur model skoring
tf_scorer = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dropout(0.4),
    # Layer output untuk regresi: 1 neuron, tanpa aktivasi (atau 'linear')
    # untuk memprediksi nilai kontinu (skor).
    Dense(1, activation='linear')
])

# Kompilasi model dengan loss dan metrik untuk regresi
tf_scorer.compile(
    optimizer='adam',
    loss='mean_squared_error',  # Loss function untuk regresi
    metrics=['mae']             # Mean Absolute Error sebagai metrik
)

tf_scorer.summary()

# Melatih model
print("\nMemulai pelatihan model skoring...")
history = tf_scorer.fit(
    X_train_score,
    y_train_score,
    epochs=15,
    batch_size=64,
    validation_data=(X_test_score, y_test_score),
    verbose=1
)

# Evaluasi model
print("\nEvaluasi Model Skoring TensorFlow")
val_loss, val_mae = tf_scorer.evaluate(X_test_score, y_test_score, verbose=0)
print(f"Mean Absolute Error (MAE) pada data uji: {val_mae:.4f}")
print(f"Ini berarti, rata-rata, prediksi skor model meleset sekitar {val_mae*100:.2f} poin dari skor sebenarnya.")

# Simpan model
tf_scorer.save('cv_jd_scorer_model.keras')
tf_scorer.save('cv_jd_scorer_model.h5')
print("\nModel skoring TensorFlow berhasil disimpan sebagai 'cv_jd_scorer_model.keras'")

# Kode Inferensi untuk Model Skoring

from tensorflow.keras.models import load_model

# Memuat model skoring yang sudah disimpan
loaded_tf_scorer = load_model('cv_jd_scorer_model.keras')
sbert_model = SentenceTransformer('all-MiniLM-L6-v2') # Pastikan sbert dimuat

def predict_score(cv_text, jd_text):
    """
    Fungsi inferensi untuk memprediksi skor kecocokan antara satu CV dan satu JD.
    """
    # 1. Preprocess & Create Embeddings
    cleaned_cv = preprocess_text_improved(cv_text)
    cleaned_jd = preprocess_text_improved(jd_text)
    cv_embedding = sbert_model.encode(cleaned_cv)
    jd_embedding = sbert_model.encode(cleaned_jd)

    # 2. Create Feature Vector (selisih)
    feature_vector = (cv_embedding - jd_embedding).reshape(1, -1)

    # 3. Predict menggunakan model skoring TensorFlow
    predicted_score = loaded_tf_scorer.predict(feature_vector)[0][0]

    # 4. Normalisasi output agar selalu dalam rentang 0-100%
    # Model linear bisa saja memprediksi sedikit di luar rentang [0,1]
    predicted_score_normalized = np.clip(predicted_score, 0, 1)

    # 5. Return result
    print(f"Skor Kecocokan yang Diprediksi: {predicted_score_normalized:.2%}")
    return predicted_score_normalized

# Penggunaan
# Menggunakan data yang sama seperti sebelumnya untuk perbandingan
cv_contoh = df_resume['CV_text'][10] # Ambil CV dari data asli
jd_contoh = df_jd['JD_text'][5]    # Ambil JD dari data asli

print("CV:", cv_contoh[:200] + "...")
print("\nJD:", jd_contoh[:200] + "...")
print("\n---")

# Memanggil fungsi inferensi yang baru
predict_score(cv_contoh, jd_contoh)

"""# **Model 2: Pemberi Umpan Balik (Generative AI dengan Vertex AI)**"""

!pip install google-cloud-aiplatform --upgrade -q

import json
import vertexai
from google.colab import auth
from google.cloud import storage
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel

try:
    auth.authenticate_user()
    PROJECT_ID = "lexical-tide-462414-s5"  # GANTI DENGAN PROJECT ID ANDA
    REGION = "us-central1"
    BUCKET_NAME = "bucket-smart-recruiter-capstonedbs" # GANTI DENGAN NAMA BUCKET ANDA
    vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=f"gs://{BUCKET_NAME}")
    print(f"✅ Terhubung ke Project: {PROJECT_ID}")
except Exception as e:
    print(f"❌ Gagal Inisialisasi: {e}")
    raise

# Cell 11: Kode Inferensi untuk Model Generatif yang Telah di Fine-Tune
# PENTING: Jalankan sel ini HANYA SETELAH pipeline job fine-tuning di atas selesai.
# Proses ini bisa memakan waktu 1-2 jam atau lebih.

TUNED_MODEL_NAME = "tuning-smart-recruiter" # Gunakan nama yang sama dengan yang di-tuning

print(f"--- Mencari model hasil fine-tuning '{TUNED_MODEL_NAME}'... ---")
try:
    # Urutkan berdasarkan waktu pembuatan untuk mendapatkan yang terbaru
    models = aiplatform.Model.list(filter=f'display_name="{TUNED_MODEL_NAME}"', order_by="create_time desc")
    if not models:
        raise Exception("Model hasil fine-tuning belum ditemukan. Pastikan job sudah selesai.")

    # Ambil model yang paling baru
    tuned_model_resource = models[0]
    print(f"✅ Model ditemukan: {tuned_model_resource.name}")

    # Buat instance model untuk inferensi
    model_instance = GenerativeModel(model_name=tuned_model_resource.name)

    # --- Contoh Penggunaan ---
    cv_input = "Saya seorang fresh graduate, belum punya pengalaman kerja tapi aktif di himpunan mahasiswa."
    jd_input = "Dibutuhkan staff administrasi yang teliti, bisa mengoperasikan Microsoft Excel, dan mampu berkomunikasi dengan baik."
    prompt = f"CV: {cv_input}\\nJOB_DESCRIPTION: {jd_input}\\nTASK: Berikan umpan balik."

    print("\\n--- Mengirim Prompt ke Model ---")
    print(prompt)

    response = model_instance.generate_content(prompt)
    feedback = response.text

    print("\\n--- UMPAN BALIK DARI MODEL GENERATIF ---")
    print(feedback)

except Exception as e:
    print(f"\\n❌ Gagal melakukan inferensi: {e}")
    print("Pastikan pipeline job fine-tuning Anda sudah selesai dengan status 'Succeeded' di Google Cloud Console.")

